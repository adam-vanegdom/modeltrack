{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as image\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import flatten\n",
    "from torch.nn import Conv2d, ReLU, MaxPool2d\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.utils\n",
    "from torchvision.io import read_image, write_png\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler, SequentialSampler, RandomSampler, Sampler\n",
    "\n",
    "import librosa\n",
    "from scipy.io.wavfile import read\n",
    "from PIL import Image\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.filters import gaussian\n",
    "from skimage.util import random_noise\n",
    "from skimage.transform import EuclideanTransform, warp\n",
    "\n",
    "import modeltrack.experiment as exp\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-776265dca4ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# import audio_intake as intake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1. DataSet and Model Classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class RawAudioDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform = None, start=None, end=None):\n",
    "        super().__init__()\n",
    "        self.img_df = dataframe\n",
    "        self.img_labels = self.img_df['impairment']\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.augment_type = dataframe['augment'] if 'augment' in dataframe else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_df)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        # create the image path and then read in the image\n",
    "        img_path = os.path.join(self.img_dir, self.img_df.iloc[index]['img_path'])\n",
    "        img = read_image(img_path)\n",
    "        label = self.img_df.iloc[index]['impairment']\n",
    "\n",
    "        # standardize the color to value between 0 and 1\n",
    "        img = torch.div(img, 255.0)\n",
    " \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            img = img.to(device=device)\n",
    "            \n",
    "        if self.augment_type is not None:\n",
    "            if self.augment_type[idx] == 'gaussian':\n",
    "                spec = gaussian(spec,sigma=0.5,multichannel=True)\n",
    "        \n",
    "        return img, label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class RawImageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RawImageModel, self).__init__()\n",
    "\n",
    "        self.vgg_16_pretrained = models.vgg16(pretrained=True)\n",
    "        \n",
    "        # want to freeze vgg layers\n",
    "        for parameter in self.vgg_16_pretrained.parameters():\n",
    "                parameter.requires_grad = False\n",
    "\n",
    "        n_inputs = self.vgg_16_pretrained.classifier[6].in_features\n",
    "        self.vgg_16_pretrained.classifier[6] = nn.Sequential(\n",
    "            nn.Linear(in_features=n_inputs, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(in_features=256, out_features=2),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        x = self.vgg_16_pretrained(img)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2. Training and Testing Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    One cycle of model training\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch_index, batch in enumerate(dataloader):\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        img, labels = batch\n",
    "        \n",
    "        # set previous gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute prediction for current batch\n",
    "        preds = model.forward(img.float())\n",
    "        \n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = loss_fn(preds, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation to compute new gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        #compute the number of correct predictions\n",
    "        total_correct += (preds.argmax(1) == labels).type(torch.float).sum().item()\n",
    "\n",
    "        # log relative information for epoch training pass\n",
    "        if batch_index % 15 == 0:\n",
    "            loss = loss.item()\n",
    "            current = batch_index * BATCH_SIZE\n",
    "            logging.info(f\"training loss: {loss:>7f}  [{current}/{len(dataloader.dataset)}]\")\n",
    "            print(f\"training loss: {loss:>7f}  [{current}/{len(dataloader.dataset)}]\")\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_train_loss = train_loss / len(dataloader)\n",
    "    train_accuracy = total_correct / len(dataloader.dataset)\n",
    "\n",
    "    return avg_train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def test_one_epoch(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    One cycle of the model testing/validation cycle\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    for batch_index, batch in enumerate(dataloader):\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        img, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model.forward(img)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            test_loss += loss.item()\n",
    "            total_correct += (preds.argmax(1) == labels).type(torch.float).sum().item()\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    avg_test_loss = test_loss / len(dataloader)\n",
    "    test_accuracy = total_correct / len(dataloader.dataset)       \n",
    "\n",
    "    return avg_test_loss, test_accuracy\n",
    "\n",
    "\n",
    "def save_checkpoint(model_name, model, epoch, optimizer, loss, is_best=False):\n",
    "        \"\"\"\n",
    "        Checkpoint saver\n",
    "        :param file_name: name of the checkpoint file\n",
    "        :param is_best: boolean flag to indicate whether current checkpoint's accuracy is the best so far\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if is_best:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                f\"../models/{model_name}/output/model_checkpoint_best.pt.tar\",\n",
    "            )\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": loss\n",
    "            },\n",
    "            f\"../models/{model_name}/checkpoint/model_checkpoint.pt.tar\",\n",
    "        )\n",
    "        \n",
    "def plot_loss(model_name, current_epoch, train_losses, test_losses, testDesc):\n",
    "        plot_file_path = f\"../models/{model_name}/output/loss_curve_{testDesc}.png\"\n",
    "        fig = plt.figure()\n",
    "        plt.plot(range(1, current_epoch + 2), train_losses, color=\"r\")\n",
    "        plt.plot(range(1, current_epoch + 2), test_losses, color=\"b\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"training loss\")\n",
    "        fig.savefig(plot_file_path, bbox_inches=\"tight\")\n",
    "        plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Running Model: Mel-Spectrogram Images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "data_files = glob.glob('../audio/*/*.wav')\n",
    "data = pd.DataFrame({'path': data_files})\n",
    "data['labels'] = data['path'].apply(lambda x: 0 if 'control' in x else 1)\n",
    "data = data.sample(frac=1, random_state=2021).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "experiment = exp.Experiment('my-first-experiment')\n",
    "experiment\n",
    "# experiment.store_params({\n",
    "#     batch: 32\n",
    "#     epochs: 100\n",
    "#     learning_rate: 1e-4\n",
    "#     device: 'cuda'\n",
    "#     seed: 2021\n",
    "#     test_size: 0.15\n",
    "# })"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'exp' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-df4a797c5cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my-first-experiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# experiment.store_params({\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     batch: 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     epochs: 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exp' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# load the data sets into their respective classes\n",
    "# Note: 0.15 train_test split was used to get respective data\n",
    "train, test = train_test_split(data, random_state=experiment.config.seed, test_size=experiment.config.test_size)\n",
    "\n",
    "training_data = SpectrogramDataset(train)\n",
    "test_data = SpectrogramDataset(test.reset_index())\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=experiment.config.batch, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=experiment.config.batch, shuffle=True)\n",
    "\n",
    "print(f\"Train Size: {len(train_loader.dataset)}, Test Size: {len(test_loader.dataset)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = RawImageModel().to(device)\n",
    "\n",
    "class_wts = compute_class_weight(\"balanced\", classes=np.unique(train['labels']), y=train['labels'])\n",
    "weights = torch.tensor(class_wts, dtype=torch.float).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=experiment.config.learning_rate)\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_test_loss = float(\"inf\")\n",
    "\n",
    "\n",
    "for current_epoch in tqdm(range(EPOCHS)):\n",
    "    print(f\"\\nEpoch {current_epoch+1}\\n-------------------------------\")\n",
    "\n",
    "    train_loss, train_acc  = train_one_epoch(train_loader, model, loss_fn, optimizer)\n",
    "    \n",
    "    test_loss, test_acc = test_one_epoch(test_loader, model, loss_fn)\n",
    "\n",
    "    experiment.save_epoch_stats(train_loss, test_loss, train_acc, test_acc)\n",
    "\n",
    "    experiment.save_model(\n",
    "        model=model,\n",
    "        epoch=current_epoch,\n",
    "        optimizer=optimizer,\n",
    "        loss=test_loss\n",
    "    )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Size: 1876, Test Size: 83\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ec2-user/anaconda3/envs/bert/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training loss: 0.698319  [0/1876]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ec2-user/SageMaker/notebooks/utils.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (arr - arr_min) / (arr_max - arr_min)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f1a850caad43468d8281508a5c6ed89c48577698d0bdbc8ffe6d84db33dc291"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('speech': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}